{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'xx', 'sample_submission.csv', 'test.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/jiweil/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import time\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import gc\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout,concatenate\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "#from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import Sequence,to_categorical\n",
    "\n",
    "#GPU = 7\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, **kwargs):\n",
    "        'Initialization'\n",
    "        self.params = kwargs\n",
    "        self.X = self.params['X']\n",
    "        self.shuffle = self.params['shuffle']\n",
    "        self.y = self.params['y']\n",
    "        self.aug = self.params['aug']\n",
    "        self.indexes = np.arange(self.y.shape[0])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        batch_size = self.params['batch_size']\n",
    "        return int(np.floor(self.indexes.shape[0] / batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        batch_size = self.params['batch_size']\n",
    "        indexes = self.indexes[index*batch_size:(index+1)*batch_size]\n",
    "\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X_base = self.X['base'][indexes]\n",
    "        X_noise1 = self.X['noise1'][indexes]\n",
    "        y = self.y[indexes]\n",
    "        if self.aug:\n",
    "            X_base,X_noise1,y = self.aug_(X_base,X_noise1,y)\n",
    "        return {'base':X_base,'noise1':X_noise1}, y\n",
    "    \n",
    "    def aug_(self,xb,xn1,y,t=2):\n",
    "        xb_pos,xb_neg,xn1_pos,xn1_neg = [],[],[],[]\n",
    "        for i in range(t):\n",
    "            mask = y>0\n",
    "            x1 = xb[mask].copy()\n",
    "            x2 = xn1[mask].copy()\n",
    "            ids = np.arange(x1.shape[0])\n",
    "            for c in range(x1.shape[1]):\n",
    "                np.random.shuffle(ids)\n",
    "                x1[:,c] = x1[ids][:,c]\n",
    "                x2[:,c] = x2[ids][:,c]\n",
    "            xb_pos.append(x1)\n",
    "            xn1_pos.append(x2)\n",
    "        \n",
    "        for i in range(t):\n",
    "            mask = y==0\n",
    "            x1 = xb[mask].copy()\n",
    "            x2 = xn1[mask].copy()\n",
    "            ids = np.arange(x1.shape[0])\n",
    "            for c in range(x1.shape[1]):\n",
    "                np.random.shuffle(ids)\n",
    "                x1[:,c] = x1[ids][:,c]\n",
    "                x2[:,c] = x2[ids][:,c]\n",
    "            xb_neg.append(x1)\n",
    "            xn1_neg.append(x2)\n",
    "    \n",
    "\n",
    "        xb_pos = np.vstack(xb_pos)\n",
    "        xb_neg = np.vstack(xb_neg)\n",
    "        xn1_pos = np.vstack(xn1_pos)\n",
    "        xn1_neg = np.vstack(xn1_neg)\n",
    "\n",
    "        ys = np.ones(xb_pos.shape[0])\n",
    "        yn = np.zeros(xb_neg.shape[0])\n",
    "        xb = np.vstack([xb,xb_pos,xb_neg])\n",
    "        xn1 = np.vstack([xn1,xn1_pos,xn1_neg])\n",
    "        y = np.concatenate([y,ys,yn])\n",
    "        return xb,xn1,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions. auc, plot_history\n",
    "def auc(y_true, y_pred):\n",
    "    #auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    y_pred = y_pred.ravel()\n",
    "    y_true = y_true.ravel()\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def auc_2(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def plot_history(histories, key='binary_crossentropy'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    #plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')\n",
    "\n",
    "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    plt.ylim([0, 0.4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_col_vals_fix(x1, groups):\n",
    "    group_size = x1.shape[1]//groups\n",
    "    xs = [x1[:, i*group_size:(i+1)*group_size] for i in range(groups)]\n",
    "    rand_x = np.array([np.random.choice(x1.shape[0], size=x1.shape[0], replace=False) for i in range(group_size)]).T\n",
    "    grid = np.indices(xs[0].shape)\n",
    "    rand_y = grid[1]\n",
    "    res = [x[(rand_x, rand_y)] for x in xs]\n",
    "    return np.hstack(res)\n",
    "\n",
    "def augment_fix_fast(x,y,groups,t1=2, t0=2):\n",
    "    # In order to make the sync version augment work, the df should be the form of:\n",
    "    # var_1, var_2, var_3 | var_1_count, var_2_count, var_3_count | var_1_rolling, var_2_rolling, var_3_rolling\n",
    "    # for the example above, 3 groups of feature, groups = 3\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t1):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        x1 = shuffle_col_vals_fix(x1, groups)\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t0):\n",
    "        mask = (y==0)\n",
    "        x1 = x[mask].copy()\n",
    "        x1 = shuffle_col_vals_fix(x1, groups)\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs); xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0]);yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn]); y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 804 ms, total: 11.2 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data \n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df =  pd.read_csv(\"../input/test.csv\")\n",
    "base_features = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 17 s, total: 31 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# mark real vs fake\n",
    "train_df['real'] = 1\n",
    "\n",
    "for col in base_features:\n",
    "    test_df[col] = test_df[col].map(test_df[col].value_counts())\n",
    "a = test_df[base_features].min(axis=1)\n",
    "\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "test_df['real'] = (a == 1).astype('int')\n",
    "\n",
    "train = train_df.append(test_df).reset_index(drop=True)\n",
    "del test_df, train_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 22.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.16 s, sys: 932 ms, total: 9.09 s\n",
      "Wall time: 9.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# count features\n",
    "for col in tqdm(base_features):\n",
    "    train[col + 'size'] = train[col].map(train.loc[train.real==1, col].value_counts())\n",
    "cnt_features = [col + 'size' for col in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:50<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 2min 50s, total: 3min 51s\n",
      "Wall time: 3min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# magice features 1\n",
    "for col in tqdm(base_features):\n",
    "#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n",
    "    train.loc[train[col+'size']>1,col+'no_noise'] = train.loc[train[col+'size']>1,col]\n",
    "noise1_features = [col + 'no_noise' for col in base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 956 ms, sys: 860 ms, total: 1.82 s\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill NA as 0, inspired by lightgbm\n",
    "train[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 436 ms, sys: 764 ms, total: 1.2 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = train[train['target'].notnull()]\n",
    "test_df = train[train['target'].isnull()]\n",
    "all_features = base_features + noise1_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "39098e416885d4b96182c53292355a0e49cb0086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.98 s, sys: 2.09 s, total: 4.06 s\n",
      "Wall time: 4.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scaler = preprocessing.StandardScaler().fit(train_df[all_features].values)\n",
    "df_trn = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)\n",
    "df_tst = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)\n",
    "y = train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(dataset, cols_info):\n",
    "    X = {}\n",
    "    base_feats, noise_feats = cols_info\n",
    "    X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))\n",
    "    X['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 ms, sys: 764 ms, total: 1.04 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols_info = [base_features, noise1_features]\n",
    "#X = get_keras_data(df_trn[all_features], cols_info)\n",
    "X_test = get_keras_data(df_tst[all_features], cols_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3afd722cdfbd3a200f5b33dcff2fe33635d02002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "base (InputLayer)               (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "noise1 (InputLayer)             (None, 200, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200, 16)      32          base[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200, 16)      32          noise1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 200, 16)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200, 16)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "base_last (Flatten)             (None, 3200)         0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "nose1_last (Flatten)            (None, 3200)         0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6400)         0           base_last[0][0]                  \n",
      "                                                                 nose1_last[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            6401        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,465\n",
      "Trainable params: 6,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define network structure -> 2D CNN\n",
    "def Convnet(cols_info, classes=1):\n",
    "    base_feats, noise1_feats = cols_info\n",
    "    \n",
    "    # base_feats\n",
    "    X_base_input = Input(shape=(len(base_feats), 1), name='base')\n",
    "    X_base = Dense(16)(X_base_input)\n",
    "    X_base = Activation('relu')(X_base)\n",
    "    X_base = Flatten(name='base_last')(X_base)\n",
    "    \n",
    "    # noise1\n",
    "    X_noise1_input = Input(shape=(len(noise1_feats), 1), name='noise1')\n",
    "    X_noise1 = Dense(16)(X_noise1_input)\n",
    "    X_noise1 = Activation('relu')(X_noise1)\n",
    "    X_noise1 = Flatten(name='nose1_last')(X_noise1)\n",
    "    \n",
    "    X = concatenate([X_base, X_noise1])\n",
    "    X = Dense(classes, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_base_input, X_noise1_input],outputs=X)\n",
    "    \n",
    "    return model\n",
    "model = Convnet(cols_info)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "d2579e2c0abf8be1f0bbe1eec545394475e37568"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del df_tst\n",
    "except:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "301805e7d06a14a7ac9087079a2eb1a839626519"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "SEED = 2019\n",
    "n_folds = 5\n",
    "debug_flag = True\n",
    "folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "471b1116f2311ea8f757ee041ec9052aebc9ca57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "234/234 [==============================] - 74s 316ms/step - loss: 0.2581 - acc: 0.9089 - binary_crossentropy: 0.2581 - auc_2: 0.8127 - val_loss: 0.2258 - val_acc: 0.9168 - val_binary_crossentropy: 0.2258 - val_auc_2: 0.8746\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.87458, saving model to nn/NN_fold1.h5\n",
      "Epoch 2/300\n",
      "234/234 [==============================] - 74s 317ms/step - loss: 0.2173 - acc: 0.9192 - binary_crossentropy: 0.2173 - auc_2: 0.8801 - val_loss: 0.2181 - val_acc: 0.9189 - val_binary_crossentropy: 0.2181 - val_auc_2: 0.8794\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.87458 to 0.87938, saving model to nn/NN_fold1.h5\n",
      "Epoch 3/300\n",
      "234/234 [==============================] - 74s 317ms/step - loss: 0.2111 - acc: 0.9220 - binary_crossentropy: 0.2111 - auc_2: 0.8874 - val_loss: 0.2075 - val_acc: 0.9234 - val_binary_crossentropy: 0.2075 - val_auc_2: 0.8929\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.87938 to 0.89289, saving model to nn/NN_fold1.h5\n",
      "Epoch 4/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1991 - acc: 0.9263 - binary_crossentropy: 0.1991 - auc_2: 0.9013 - val_loss: 0.1962 - val_acc: 0.9270 - val_binary_crossentropy: 0.1962 - val_auc_2: 0.9060\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.89289 to 0.90598, saving model to nn/NN_fold1.h5\n",
      "Epoch 5/300\n",
      "234/234 [==============================] - 72s 309ms/step - loss: 0.1909 - acc: 0.9294 - binary_crossentropy: 0.1909 - auc_2: 0.9105 - val_loss: 0.1902 - val_acc: 0.9289 - val_binary_crossentropy: 0.1902 - val_auc_2: 0.9125\n",
      "\n",
      "Epoch 00005: val_auc_2 improved from 0.90598 to 0.91255, saving model to nn/NN_fold1.h5\n",
      "Epoch 6/300\n",
      "234/234 [==============================] - 74s 317ms/step - loss: 0.1866 - acc: 0.9311 - binary_crossentropy: 0.1866 - auc_2: 0.9147 - val_loss: 0.1884 - val_acc: 0.9291 - val_binary_crossentropy: 0.1884 - val_auc_2: 0.9153\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.91255 to 0.91534, saving model to nn/NN_fold1.h5\n",
      "Epoch 7/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1838 - acc: 0.9322 - binary_crossentropy: 0.1838 - auc_2: 0.9172 - val_loss: 0.1845 - val_acc: 0.9315 - val_binary_crossentropy: 0.1845 - val_auc_2: 0.9174\n",
      "\n",
      "Epoch 00007: val_auc_2 improved from 0.91534 to 0.91742, saving model to nn/NN_fold1.h5\n",
      "Epoch 8/300\n",
      "234/234 [==============================] - 73s 311ms/step - loss: 0.1817 - acc: 0.9329 - binary_crossentropy: 0.1817 - auc_2: 0.9194 - val_loss: 0.1828 - val_acc: 0.9325 - val_binary_crossentropy: 0.1828 - val_auc_2: 0.9189\n",
      "\n",
      "Epoch 00008: val_auc_2 improved from 0.91742 to 0.91893, saving model to nn/NN_fold1.h5\n",
      "Epoch 9/300\n",
      "234/234 [==============================] - 72s 308ms/step - loss: 0.1802 - acc: 0.9334 - binary_crossentropy: 0.1802 - auc_2: 0.9210 - val_loss: 0.1817 - val_acc: 0.9327 - val_binary_crossentropy: 0.1817 - val_auc_2: 0.9199\n",
      "\n",
      "Epoch 00009: val_auc_2 improved from 0.91893 to 0.91986, saving model to nn/NN_fold1.h5\n",
      "Epoch 10/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1788 - acc: 0.9342 - binary_crossentropy: 0.1788 - auc_2: 0.9216 - val_loss: 0.1805 - val_acc: 0.9334 - val_binary_crossentropy: 0.1805 - val_auc_2: 0.9208\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.91986 to 0.92083, saving model to nn/NN_fold1.h5\n",
      "Epoch 11/300\n",
      "234/234 [==============================] - 74s 314ms/step - loss: 0.1779 - acc: 0.9343 - binary_crossentropy: 0.1779 - auc_2: 0.9228 - val_loss: 0.1801 - val_acc: 0.9336 - val_binary_crossentropy: 0.1801 - val_auc_2: 0.9214\n",
      "\n",
      "Epoch 00011: val_auc_2 improved from 0.92083 to 0.92139, saving model to nn/NN_fold1.h5\n",
      "Epoch 12/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1772 - acc: 0.9347 - binary_crossentropy: 0.1772 - auc_2: 0.9231 - val_loss: 0.1794 - val_acc: 0.9338 - val_binary_crossentropy: 0.1794 - val_auc_2: 0.9218\n",
      "\n",
      "Epoch 00012: val_auc_2 improved from 0.92139 to 0.92177, saving model to nn/NN_fold1.h5\n",
      "Epoch 13/300\n",
      "234/234 [==============================] - 74s 314ms/step - loss: 0.1767 - acc: 0.9347 - binary_crossentropy: 0.1767 - auc_2: 0.9240 - val_loss: 0.1788 - val_acc: 0.9340 - val_binary_crossentropy: 0.1788 - val_auc_2: 0.9223\n",
      "\n",
      "Epoch 00013: val_auc_2 improved from 0.92177 to 0.92225, saving model to nn/NN_fold1.h5\n",
      "Epoch 14/300\n",
      "234/234 [==============================] - 73s 311ms/step - loss: 0.1760 - acc: 0.9352 - binary_crossentropy: 0.1760 - auc_2: 0.9243 - val_loss: 0.1784 - val_acc: 0.9341 - val_binary_crossentropy: 0.1784 - val_auc_2: 0.9226\n",
      "\n",
      "Epoch 00014: val_auc_2 improved from 0.92225 to 0.92261, saving model to nn/NN_fold1.h5\n",
      "Epoch 15/300\n",
      "234/234 [==============================] - 74s 314ms/step - loss: 0.1758 - acc: 0.9353 - binary_crossentropy: 0.1758 - auc_2: 0.9244 - val_loss: 0.1784 - val_acc: 0.9347 - val_binary_crossentropy: 0.1784 - val_auc_2: 0.9225\n",
      "\n",
      "Epoch 00015: val_auc_2 did not improve from 0.92261\n",
      "Epoch 16/300\n",
      "234/234 [==============================] - 72s 309ms/step - loss: 0.1750 - acc: 0.9354 - binary_crossentropy: 0.1750 - auc_2: 0.9251 - val_loss: 0.1800 - val_acc: 0.9335 - val_binary_crossentropy: 0.1800 - val_auc_2: 0.9228\n",
      "\n",
      "Epoch 00016: val_auc_2 improved from 0.92261 to 0.92275, saving model to nn/NN_fold1.h5\n",
      "Epoch 17/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1749 - acc: 0.9354 - binary_crossentropy: 0.1749 - auc_2: 0.9253 - val_loss: 0.1781 - val_acc: 0.9341 - val_binary_crossentropy: 0.1781 - val_auc_2: 0.9230\n",
      "\n",
      "Epoch 00017: val_auc_2 improved from 0.92275 to 0.92300, saving model to nn/NN_fold1.h5\n",
      "Epoch 18/300\n",
      "234/234 [==============================] - 75s 319ms/step - loss: 0.1746 - acc: 0.9356 - binary_crossentropy: 0.1746 - auc_2: 0.9256 - val_loss: 0.1779 - val_acc: 0.9348 - val_binary_crossentropy: 0.1779 - val_auc_2: 0.9229\n",
      "\n",
      "Epoch 00018: val_auc_2 did not improve from 0.92300\n",
      "Epoch 19/300\n",
      "234/234 [==============================] - 74s 314ms/step - loss: 0.1744 - acc: 0.9355 - binary_crossentropy: 0.1744 - auc_2: 0.9259 - val_loss: 0.1776 - val_acc: 0.9347 - val_binary_crossentropy: 0.1776 - val_auc_2: 0.9232\n",
      "\n",
      "Epoch 00019: val_auc_2 improved from 0.92300 to 0.92316, saving model to nn/NN_fold1.h5\n",
      "Epoch 20/300\n",
      "234/234 [==============================] - 75s 320ms/step - loss: 0.1743 - acc: 0.9357 - binary_crossentropy: 0.1743 - auc_2: 0.9257 - val_loss: 0.1785 - val_acc: 0.9335 - val_binary_crossentropy: 0.1785 - val_auc_2: 0.9232\n",
      "\n",
      "Epoch 00020: val_auc_2 improved from 0.92316 to 0.92319, saving model to nn/NN_fold1.h5\n",
      "Epoch 21/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1740 - acc: 0.9358 - binary_crossentropy: 0.1740 - auc_2: 0.9262 - val_loss: 0.1774 - val_acc: 0.9344 - val_binary_crossentropy: 0.1774 - val_auc_2: 0.9233\n",
      "\n",
      "Epoch 00021: val_auc_2 improved from 0.92319 to 0.92327, saving model to nn/NN_fold1.h5\n",
      "Epoch 22/300\n",
      "234/234 [==============================] - 72s 308ms/step - loss: 0.1738 - acc: 0.9360 - binary_crossentropy: 0.1738 - auc_2: 0.9261 - val_loss: 0.1786 - val_acc: 0.9340 - val_binary_crossentropy: 0.1786 - val_auc_2: 0.9231\n",
      "\n",
      "Epoch 00022: val_auc_2 did not improve from 0.92327\n",
      "Epoch 23/300\n",
      "234/234 [==============================] - 74s 314ms/step - loss: 0.1737 - acc: 0.9360 - binary_crossentropy: 0.1737 - auc_2: 0.9262 - val_loss: 0.1775 - val_acc: 0.9350 - val_binary_crossentropy: 0.1775 - val_auc_2: 0.9230\n",
      "\n",
      "Epoch 00023: val_auc_2 did not improve from 0.92327\n",
      "Epoch 24/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1735 - acc: 0.9360 - binary_crossentropy: 0.1735 - auc_2: 0.9267 - val_loss: 0.1779 - val_acc: 0.9345 - val_binary_crossentropy: 0.1779 - val_auc_2: 0.9232\n",
      "\n",
      "Epoch 00024: val_auc_2 did not improve from 0.92327\n",
      "Epoch 25/300\n",
      "234/234 [==============================] - 74s 316ms/step - loss: 0.1733 - acc: 0.9360 - binary_crossentropy: 0.1733 - auc_2: 0.9269 - val_loss: 0.1773 - val_acc: 0.9347 - val_binary_crossentropy: 0.1773 - val_auc_2: 0.9232\n",
      "\n",
      "Epoch 00025: val_auc_2 did not improve from 0.92327\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 26/300\n",
      "234/234 [==============================] - 72s 310ms/step - loss: 0.1725 - acc: 0.9364 - binary_crossentropy: 0.1725 - auc_2: 0.9272 - val_loss: 0.1770 - val_acc: 0.9352 - val_binary_crossentropy: 0.1770 - val_auc_2: 0.9235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_auc_2 improved from 0.92327 to 0.92348, saving model to nn/NN_fold1.h5\n",
      "Epoch 27/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1726 - acc: 0.9363 - binary_crossentropy: 0.1726 - auc_2: 0.9271 - val_loss: 0.1772 - val_acc: 0.9348 - val_binary_crossentropy: 0.1772 - val_auc_2: 0.9234\n",
      "\n",
      "Epoch 00027: val_auc_2 did not improve from 0.92348\n",
      "Epoch 28/300\n",
      "234/234 [==============================] - 75s 320ms/step - loss: 0.1727 - acc: 0.9363 - binary_crossentropy: 0.1727 - auc_2: 0.9271 - val_loss: 0.1769 - val_acc: 0.9349 - val_binary_crossentropy: 0.1769 - val_auc_2: 0.9236\n",
      "\n",
      "Epoch 00028: val_auc_2 improved from 0.92348 to 0.92365, saving model to nn/NN_fold1.h5\n",
      "Epoch 29/300\n",
      "234/234 [==============================] - 74s 317ms/step - loss: 0.1724 - acc: 0.9363 - binary_crossentropy: 0.1724 - auc_2: 0.9274 - val_loss: 0.1771 - val_acc: 0.9349 - val_binary_crossentropy: 0.1771 - val_auc_2: 0.9234\n",
      "\n",
      "Epoch 00029: val_auc_2 did not improve from 0.92365\n",
      "Epoch 30/300\n",
      "234/234 [==============================] - 73s 314ms/step - loss: 0.1724 - acc: 0.9364 - binary_crossentropy: 0.1724 - auc_2: 0.9276 - val_loss: 0.1774 - val_acc: 0.9343 - val_binary_crossentropy: 0.1774 - val_auc_2: 0.9232\n",
      "\n",
      "Epoch 00030: val_auc_2 did not improve from 0.92365\n",
      "Epoch 31/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1728 - acc: 0.9362 - binary_crossentropy: 0.1728 - auc_2: 0.9270 - val_loss: 0.1771 - val_acc: 0.9348 - val_binary_crossentropy: 0.1771 - val_auc_2: 0.9235\n",
      "\n",
      "Epoch 00031: val_auc_2 did not improve from 0.92365\n",
      "Epoch 32/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1726 - acc: 0.9363 - binary_crossentropy: 0.1726 - auc_2: 0.9272 - val_loss: 0.1771 - val_acc: 0.9347 - val_binary_crossentropy: 0.1771 - val_auc_2: 0.9236\n",
      "\n",
      "Epoch 00032: val_auc_2 did not improve from 0.92365\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 33/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1720 - acc: 0.9364 - binary_crossentropy: 0.1720 - auc_2: 0.9279 - val_loss: 0.1771 - val_acc: 0.9349 - val_binary_crossentropy: 0.1771 - val_auc_2: 0.9236\n",
      "\n",
      "Epoch 00033: val_auc_2 did not improve from 0.92365\n",
      "Epoch 34/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1720 - acc: 0.9365 - binary_crossentropy: 0.1720 - auc_2: 0.9277 - val_loss: 0.1769 - val_acc: 0.9348 - val_binary_crossentropy: 0.1769 - val_auc_2: 0.9236\n",
      "\n",
      "Epoch 00034: val_auc_2 did not improve from 0.92365\n",
      "Epoch 35/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1723 - acc: 0.9364 - binary_crossentropy: 0.1723 - auc_2: 0.9276 - val_loss: 0.1769 - val_acc: 0.9346 - val_binary_crossentropy: 0.1769 - val_auc_2: 0.9236\n",
      "\n",
      "Epoch 00035: val_auc_2 did not improve from 0.92365\n",
      "Epoch 36/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1720 - acc: 0.9365 - binary_crossentropy: 0.1720 - auc_2: 0.9279 - val_loss: 0.1773 - val_acc: 0.9346 - val_binary_crossentropy: 0.1773 - val_auc_2: 0.9235\n",
      "\n",
      "Epoch 00036: val_auc_2 did not improve from 0.92365\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 37/300\n",
      "234/234 [==============================] - 74s 318ms/step - loss: 0.1718 - acc: 0.9365 - binary_crossentropy: 0.1718 - auc_2: 0.9280 - val_loss: 0.1771 - val_acc: 0.9348 - val_binary_crossentropy: 0.1771 - val_auc_2: 0.9235\n",
      "\n",
      "Epoch 00037: val_auc_2 did not improve from 0.92365\n",
      "Epoch 38/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1716 - acc: 0.9367 - binary_crossentropy: 0.1716 - auc_2: 0.9279 - val_loss: 0.1770 - val_acc: 0.9347 - val_binary_crossentropy: 0.1770 - val_auc_2: 0.9235\n",
      "\n",
      "Epoch 00038: val_auc_2 did not improve from 0.92365\n",
      "Epoch 00038: early stopping\n",
      "Epoch 1/300\n",
      "234/234 [==============================] - 74s 318ms/step - loss: 0.2607 - acc: 0.9082 - binary_crossentropy: 0.2607 - auc_2: 0.7997 - val_loss: 0.2209 - val_acc: 0.9185 - val_binary_crossentropy: 0.2209 - val_auc_2: 0.8744\n",
      "\n",
      "Epoch 00001: val_auc_2 improved from -inf to 0.87437, saving model to nn/NN_fold2.h5\n",
      "Epoch 2/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.2184 - acc: 0.9192 - binary_crossentropy: 0.2184 - auc_2: 0.8782 - val_loss: 0.2170 - val_acc: 0.9200 - val_binary_crossentropy: 0.2170 - val_auc_2: 0.8797\n",
      "\n",
      "Epoch 00002: val_auc_2 improved from 0.87437 to 0.87966, saving model to nn/NN_fold2.h5\n",
      "Epoch 3/300\n",
      "234/234 [==============================] - 72s 306ms/step - loss: 0.2127 - acc: 0.9212 - binary_crossentropy: 0.2127 - auc_2: 0.8852 - val_loss: 0.2107 - val_acc: 0.9228 - val_binary_crossentropy: 0.2107 - val_auc_2: 0.8888\n",
      "\n",
      "Epoch 00003: val_auc_2 improved from 0.87966 to 0.88884, saving model to nn/NN_fold2.h5\n",
      "Epoch 4/300\n",
      "234/234 [==============================] - 71s 304ms/step - loss: 0.2017 - acc: 0.9254 - binary_crossentropy: 0.2017 - auc_2: 0.8986 - val_loss: 0.1967 - val_acc: 0.9275 - val_binary_crossentropy: 0.1967 - val_auc_2: 0.9029\n",
      "\n",
      "Epoch 00004: val_auc_2 improved from 0.88884 to 0.90286, saving model to nn/NN_fold2.h5\n",
      "Epoch 5/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1923 - acc: 0.9288 - binary_crossentropy: 0.1923 - auc_2: 0.9091 - val_loss: 0.1899 - val_acc: 0.9306 - val_binary_crossentropy: 0.1899 - val_auc_2: 0.9102\n",
      "\n",
      "Epoch 00005: val_auc_2 improved from 0.90286 to 0.91025, saving model to nn/NN_fold2.h5\n",
      "Epoch 6/300\n",
      "234/234 [==============================] - 71s 303ms/step - loss: 0.1871 - acc: 0.9308 - binary_crossentropy: 0.1871 - auc_2: 0.9143 - val_loss: 0.1867 - val_acc: 0.9314 - val_binary_crossentropy: 0.1867 - val_auc_2: 0.9131\n",
      "\n",
      "Epoch 00006: val_auc_2 improved from 0.91025 to 0.91311, saving model to nn/NN_fold2.h5\n",
      "Epoch 7/300\n",
      "234/234 [==============================] - 72s 310ms/step - loss: 0.1842 - acc: 0.9320 - binary_crossentropy: 0.1842 - auc_2: 0.9169 - val_loss: 0.1844 - val_acc: 0.9328 - val_binary_crossentropy: 0.1844 - val_auc_2: 0.9153\n",
      "\n",
      "Epoch 00007: val_auc_2 improved from 0.91311 to 0.91531, saving model to nn/NN_fold2.h5\n",
      "Epoch 8/300\n",
      "234/234 [==============================] - 72s 308ms/step - loss: 0.1823 - acc: 0.9326 - binary_crossentropy: 0.1823 - auc_2: 0.9191 - val_loss: 0.1832 - val_acc: 0.9336 - val_binary_crossentropy: 0.1832 - val_auc_2: 0.9166\n",
      "\n",
      "Epoch 00008: val_auc_2 improved from 0.91531 to 0.91665, saving model to nn/NN_fold2.h5\n",
      "Epoch 9/300\n",
      "234/234 [==============================] - 74s 316ms/step - loss: 0.1805 - acc: 0.9334 - binary_crossentropy: 0.1805 - auc_2: 0.9203 - val_loss: 0.1827 - val_acc: 0.9341 - val_binary_crossentropy: 0.1827 - val_auc_2: 0.9175\n",
      "\n",
      "Epoch 00009: val_auc_2 improved from 0.91665 to 0.91754, saving model to nn/NN_fold2.h5\n",
      "Epoch 10/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1794 - acc: 0.9339 - binary_crossentropy: 0.1794 - auc_2: 0.9214 - val_loss: 0.1809 - val_acc: 0.9337 - val_binary_crossentropy: 0.1809 - val_auc_2: 0.9183\n",
      "\n",
      "Epoch 00010: val_auc_2 improved from 0.91754 to 0.91832, saving model to nn/NN_fold2.h5\n",
      "Epoch 11/300\n",
      "234/234 [==============================] - 72s 309ms/step - loss: 0.1784 - acc: 0.9343 - binary_crossentropy: 0.1784 - auc_2: 0.9219 - val_loss: 0.1802 - val_acc: 0.9343 - val_binary_crossentropy: 0.1802 - val_auc_2: 0.9190\n",
      "\n",
      "Epoch 00011: val_auc_2 improved from 0.91832 to 0.91902, saving model to nn/NN_fold2.h5\n",
      "Epoch 12/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1774 - acc: 0.9345 - binary_crossentropy: 0.1774 - auc_2: 0.9231 - val_loss: 0.1808 - val_acc: 0.9346 - val_binary_crossentropy: 0.1808 - val_auc_2: 0.9195\n",
      "\n",
      "Epoch 00012: val_auc_2 improved from 0.91902 to 0.91954, saving model to nn/NN_fold2.h5\n",
      "Epoch 13/300\n",
      "234/234 [==============================] - 72s 308ms/step - loss: 0.1771 - acc: 0.9349 - binary_crossentropy: 0.1771 - auc_2: 0.9233 - val_loss: 0.1792 - val_acc: 0.9352 - val_binary_crossentropy: 0.1792 - val_auc_2: 0.9200\n",
      "\n",
      "Epoch 00013: val_auc_2 improved from 0.91954 to 0.92001, saving model to nn/NN_fold2.h5\n",
      "Epoch 14/300\n",
      "234/234 [==============================] - 74s 315ms/step - loss: 0.1767 - acc: 0.9347 - binary_crossentropy: 0.1767 - auc_2: 0.9240 - val_loss: 0.1789 - val_acc: 0.9345 - val_binary_crossentropy: 0.1789 - val_auc_2: 0.9203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00014: val_auc_2 improved from 0.92001 to 0.92030, saving model to nn/NN_fold2.h5\n",
      "Epoch 15/300\n",
      "234/234 [==============================] - 73s 310ms/step - loss: 0.1760 - acc: 0.9351 - binary_crossentropy: 0.1760 - auc_2: 0.9243 - val_loss: 0.1787 - val_acc: 0.9352 - val_binary_crossentropy: 0.1787 - val_auc_2: 0.9206\n",
      "\n",
      "Epoch 00015: val_auc_2 improved from 0.92030 to 0.92060, saving model to nn/NN_fold2.h5\n",
      "Epoch 16/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1754 - acc: 0.9352 - binary_crossentropy: 0.1754 - auc_2: 0.9252 - val_loss: 0.1818 - val_acc: 0.9338 - val_binary_crossentropy: 0.1818 - val_auc_2: 0.9208\n",
      "\n",
      "Epoch 00016: val_auc_2 improved from 0.92060 to 0.92078, saving model to nn/NN_fold2.h5\n",
      "Epoch 17/300\n",
      "234/234 [==============================] - 73s 312ms/step - loss: 0.1750 - acc: 0.9354 - binary_crossentropy: 0.1750 - auc_2: 0.9253 - val_loss: 0.1785 - val_acc: 0.9349 - val_binary_crossentropy: 0.1785 - val_auc_2: 0.9210\n",
      "\n",
      "Epoch 00017: val_auc_2 improved from 0.92078 to 0.92104, saving model to nn/NN_fold2.h5\n",
      "Epoch 18/300\n",
      "234/234 [==============================] - 72s 308ms/step - loss: 0.1750 - acc: 0.9355 - binary_crossentropy: 0.1750 - auc_2: 0.9252 - val_loss: 0.1781 - val_acc: 0.9354 - val_binary_crossentropy: 0.1781 - val_auc_2: 0.9210\n",
      "\n",
      "Epoch 00018: val_auc_2 did not improve from 0.92104\n",
      "Epoch 19/300\n",
      "234/234 [==============================] - 72s 310ms/step - loss: 0.1747 - acc: 0.9356 - binary_crossentropy: 0.1747 - auc_2: 0.9257 - val_loss: 0.1782 - val_acc: 0.9351 - val_binary_crossentropy: 0.1782 - val_auc_2: 0.9211\n",
      "\n",
      "Epoch 00019: val_auc_2 improved from 0.92104 to 0.92113, saving model to nn/NN_fold2.h5\n",
      "Epoch 20/300\n",
      "234/234 [==============================] - 71s 304ms/step - loss: 0.1746 - acc: 0.9356 - binary_crossentropy: 0.1746 - auc_2: 0.9256 - val_loss: 0.1781 - val_acc: 0.9351 - val_binary_crossentropy: 0.1781 - val_auc_2: 0.9211\n",
      "\n",
      "Epoch 00020: val_auc_2 did not improve from 0.92113\n",
      "Epoch 21/300\n",
      "234/234 [==============================] - 72s 307ms/step - loss: 0.1745 - acc: 0.9357 - binary_crossentropy: 0.1745 - auc_2: 0.9257 - val_loss: 0.1777 - val_acc: 0.9352 - val_binary_crossentropy: 0.1777 - val_auc_2: 0.9217\n",
      "\n",
      "Epoch 00021: val_auc_2 improved from 0.92113 to 0.92170, saving model to nn/NN_fold2.h5\n",
      "Epoch 22/300\n",
      "234/234 [==============================] - 73s 313ms/step - loss: 0.1742 - acc: 0.9357 - binary_crossentropy: 0.1742 - auc_2: 0.9261 - val_loss: 0.1779 - val_acc: 0.9358 - val_binary_crossentropy: 0.1779 - val_auc_2: 0.9215\n",
      "\n",
      "Epoch 00022: val_auc_2 did not improve from 0.92170\n",
      "Epoch 23/300\n",
      "234/234 [==============================] - 73s 311ms/step - loss: 0.1739 - acc: 0.9358 - binary_crossentropy: 0.1739 - auc_2: 0.9262 - val_loss: 0.1782 - val_acc: 0.9351 - val_binary_crossentropy: 0.1782 - val_auc_2: 0.9213\n",
      "\n",
      "Epoch 00023: val_auc_2 did not improve from 0.92170\n",
      "Epoch 24/300\n",
      "234/234 [==============================] - 74s 314ms/step - loss: 0.1739 - acc: 0.9359 - binary_crossentropy: 0.1739 - auc_2: 0.9259 - val_loss: 0.1778 - val_acc: 0.9354 - val_binary_crossentropy: 0.1778 - val_auc_2: 0.9216\n",
      "\n",
      "Epoch 00024: val_auc_2 did not improve from 0.92170\n",
      "Epoch 25/300\n",
      "234/234 [==============================] - 73s 314ms/step - loss: 0.1739 - acc: 0.9356 - binary_crossentropy: 0.1739 - auc_2: 0.9265 - val_loss: 0.1783 - val_acc: 0.9349 - val_binary_crossentropy: 0.1783 - val_auc_2: 0.9214\n",
      "\n",
      "Epoch 00025: val_auc_2 did not improve from 0.92170\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 26/300\n",
      "234/234 [==============================] - 73s 310ms/step - loss: 0.1731 - acc: 0.9361 - binary_crossentropy: 0.1731 - auc_2: 0.9270 - val_loss: 0.1777 - val_acc: 0.9348 - val_binary_crossentropy: 0.1777 - val_auc_2: 0.9218\n",
      "\n",
      "Epoch 00026: val_auc_2 improved from 0.92170 to 0.92176, saving model to nn/NN_fold2.h5\n",
      "Epoch 27/300\n",
      "234/234 [==============================] - 73s 310ms/step - loss: 0.1729 - acc: 0.9362 - binary_crossentropy: 0.1729 - auc_2: 0.9271 - val_loss: 0.1777 - val_acc: 0.9351 - val_binary_crossentropy: 0.1777 - val_auc_2: 0.9217\n",
      "\n",
      "Epoch 00027: val_auc_2 did not improve from 0.92176\n",
      "Epoch 28/300\n",
      "129/234 [===============>..............] - ETA: 32s - loss: 0.1728 - acc: 0.9361 - binary_crossentropy: 0.1728 - auc_2: 0.9272"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#transformed_shape = tuple([-1] + list(shape))\n",
    "#X_test = np.reshape(X_test, transformed_shape)\n",
    "\n",
    "i = 0\n",
    "result = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "val_aucs = []\n",
    "valid_X = train_df[['target']]\n",
    "valid_X['predict'] = 0\n",
    "for train_idx, val_idx in skf.split(df_trn, y):\n",
    "    if i == folds:\n",
    "        break\n",
    "    i += 1    \n",
    "    X_train, y_train = df_trn.iloc[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = df_trn.iloc[val_idx], y[val_idx]\n",
    "    \n",
    "    #aug\n",
    "    X_train, y_train = augment_fix_fast(X_train.values, y_train, groups=2, t1=2, t0=2)\n",
    "    X_train = pd.DataFrame(X_train, columns=all_features)\n",
    "    \n",
    "    X_train = get_keras_data(X_train, cols_info)\n",
    "    X_valid = get_keras_data(X_valid, cols_info)\n",
    "    #X_train = np.reshape(X_train, transformed_shape)\n",
    "    #X_valid = np.reshape(X_valid, transformed_shape)\n",
    "    \n",
    "    model_name = 'nn/NN_fold{}.h5'.format(str(i))\n",
    "    \n",
    "    model = Convnet(cols_info)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])\n",
    "    checkpoint = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1, \n",
    "                                 save_best_only=True, mode='max', save_weights_only = True)\n",
    "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                       verbose=1, mode='min', epsilon=0.0001)\n",
    "    earlystop = EarlyStopping(monitor='val_auc_2', mode='max', patience=10, verbose=1)\n",
    "    \n",
    "    if 0:\n",
    "        history = model.fit(X_train, y_train, \n",
    "                        epochs=300, \n",
    "                        batch_size=1024 * 2, \n",
    "                        validation_data=(X_valid, y_valid), \n",
    "                        callbacks=[checkpoint, reduceLROnPlat, earlystop])\n",
    "    else:\n",
    "        training_generator = DataGenerator(X=X_train,y=y_train,aug=1,batch_size=1024*2,shuffle=True)\n",
    "        validation_generator = DataGenerator(X=X_valid,y=y_valid,aug=0,batch_size=1024*2,shuffle=False)\n",
    "        history = model.fit_generator(generator=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=300,  \n",
    "                        callbacks=[checkpoint, reduceLROnPlat, earlystop])\n",
    "    train_history = pd.DataFrame(history.history)\n",
    "    train_history.to_csv('nn/train_profile_fold{}.csv'.format(str(i)), index=None)\n",
    "    \n",
    "    # load and predict\n",
    "    model.load_weights(model_name)\n",
    "    \n",
    "    #predict\n",
    "    y_pred_keras = model.predict(X_valid).ravel()\n",
    "    \n",
    "    # AUC\n",
    "    valid_X['predict'].iloc[val_idx] = y_pred_keras\n",
    "    \n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_valid, y_pred_keras)\n",
    "    auc_valid = roc_auc_score(y_valid, y_pred_keras)\n",
    "    val_aucs.append(auc_valid)\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    result[\"fold{}\".format(str(i))] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold_1 AUC: 0.922040\n",
      "Fold_2 AUC: 0.921082\n",
      "Fold_3 AUC: 0.924821\n",
      "Fold_4 AUC: 0.921676\n",
      "Fold_5 AUC: 0.921367\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_aucs)):\n",
    "    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "cf12c8076b868e0f1228fd2884b14f86a87c0c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold_1 AUC: 0.921823\n",
      "Fold_2 AUC: 0.920845\n",
      "Fold_3 AUC: 0.924355\n",
      "Fold_4 AUC: 0.921661\n",
      "Fold_5 AUC: 0.921352\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_aucs)):\n",
    "    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold auc mean: 0.922197336, std: 0.001349837. All auc: 0.922151.\n"
     ]
    }
   ],
   "source": [
    "# summary on results\n",
    "auc_mean = np.mean(val_aucs)\n",
    "auc_std = np.std(val_aucs)\n",
    "auc_all = roc_auc_score(valid_X.target, valid_X.predict)\n",
    "print('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "f5b28c3ad1a96e4edd711546667cbac1527d57c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold auc mean: 0.922007239, std: 0.001220462. All auc: 0.921973.\n"
     ]
    }
   ],
   "source": [
    "# summary on results\n",
    "auc_mean = np.mean(val_aucs)\n",
    "auc_std = np.std(val_aucs)\n",
    "auc_all = roc_auc_score(valid_X.target, valid_X.predict)\n",
    "print('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "e978483f1836a293a76bcf54d27cec81905a3667"
   },
   "outputs": [],
   "source": [
    "y_all = result.values[:, 1:]\n",
    "result['target'] = np.mean(y_all, axis = 1)\n",
    "to_submit = result[['ID_code', 'target']]\n",
    "to_submit.to_csv('NN_submission.csv', index=None)\n",
    "result.to_csv('nn/NN_all_prediction.csv', index=None)\n",
    "valid_X['ID_code'] = train_df['ID_code']\n",
    "valid_X = valid_X[['ID_code', 'target', 'predict']].to_csv('nn/NN_oof.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
